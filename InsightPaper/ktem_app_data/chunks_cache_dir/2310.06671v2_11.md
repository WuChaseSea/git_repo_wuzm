Page label: 5
File name: 2310.06671v2.pdf
text:
In practice, the adapter Pwould be a simple projection layer
[50]. Then we putKin the front of the original input sequence
Sserving as a prefix of the instruction and triple prompt Sùëòùëùùëé=
K‚äïIùëñùë°‚äïX. This way, all the following text tokens can be seen
with the prefixKdue to the unidirectional attention in decoder-
only LLMs. By doing this, the textual tokens can pay unidirectional
attention to the structural embeddings of the input triple. Such
a structure-aware prompt will be employed during fine-tuning
and inference. During training, we froze the pre-trained structural
embeddings. The adapter is optimized to learn the mapping from
structural knowledge toward textual representation and will have
the generalization to new triples in the inference stage, which will
benefit the textual description and provide the triple information
from another perspective to make enhanced predictions.Table 2: Statistical information of datasets. The positve (+)
and negative (-) samples are 1:1 in the valid / test set.
Dataset|E| |R| #Train #Valid(+/-) #Test(+/-)
UMLS 135 46 5216 652/652 661/661
CoDeX-S 2034 42 32888 1827/1827 1828/1828
FB15K-237N 13104 93 87282 7041/7041 8226/8226
4.3 Complexity Analysis
After proposing KoPA, we make a comparison among LLM-based
KGC methods to demonstrate the advantages of KoPA, which is
shown in Table 1. Compared with the basic paradigms (ZSR/ICL/IT),
KoPA incorporates the KG structural embeddings into LLM to
combine the textual and structural information. Meanwhile, KoPA
makes the length of the prompt more refined as the length of virtual
tokens generated by the structural prefix adapter is fixed to 3 for
head/relation/tail respectively. In contrast, the prompt length of
structure-aware IT (enhanced IT in the table) is linearly related to
the number of neighborhood triples ùëò. In contrast to methods that
incorporate structural information based on textual descriptions,
KoPA achieves this goal by fixed-length virtual knowledge tokens
generated by the adapter.
5 EXPERIMENTS
5.1 Experimental Settings
5.1.1 Datasets .In our experiments, we use three public KG bench-
marks UMLS [ 40], CoDeX-S [ 28], and FB15K-237N [ 21] to evaluate
the proposed LLM-based KGC methods. The detailed split informa-
tion of the datasets is shown in Table 2.
5.1.2 Baseline Methods .In our experiments, we provide a com-
prehensive comparison with three broad classes of baselines on
triple classification, which is an important subtask of KGC. The
KGC baselines can be divided into three parts: embedding-based
methods [ 3,31,34,38], PLM-based methods [ 21,40], and LLM-based
methods [ 41]. Besides, we further divide the LLM-based methods
into two categories: training-free methods and fine-tuning methods.
Training-free methods consist of ZSR and ICL while fine-tuning
methods consist of vanilla IT and structure-aware IT (enhanced IT).
The specific models used for these baselines are listed below:
(1).Embedding-based KGC methods . We select four tradi-
tional embedding-based KGC methods for comparisons, namely
TransE [], DistMult [ 38], ComplEx [ 34], and RotatE [ 31]. These
methods predict the triple plausibility by the learned structural
embeddings and the score functions defined in the model.
(2).PLM-based KGC methods . We select KG-BERT [ 40] and