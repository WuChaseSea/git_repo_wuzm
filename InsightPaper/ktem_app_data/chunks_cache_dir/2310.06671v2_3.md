Page label: 2
File name: 2310.06671v2.pdf
text:
•Designing new cross-modal LLM paradigm. We further pro-
pose a knowledge prefix adapter (KoPA) that effectively inte-
grates pre-trained KG structural embeddings with LLMs. KoPA
fosters comprehensive cross-modal interactions between tex-
tual embeddings from LLMs and structural embeddings sourced
from KGs to enhance LLM’s reasoning ability.
•Comprehensive evaluation. We conduct extensive experi-
ments on three public benchmarks and evaluate the KGC per-
formance of all the structure-aware methods proposed by us
with adequate baseline comparison with further exploration of
the transfer ability and knowledge retention degree.2 RELATED WORKS
2.1 Knowledge Graph Completion
Knowledge graph completion (KGC) [ 37] is an important topic in
the KG community, aiming to mine unobserved triples in a given
KG. KGC contains several sub-tasks such as triple classification
[3], entity prediction [ 3]. The common point among KGC tasks is
to establish an effective mechanism to measure the plausibility of
the triples. The mainstream KGC methods can be divided into two
categories: embedding-based and PLM-based methods. Embedding-
based methods [ 3,31,34,38] are designed to embed the entities
and relations of KGs into continuous representation spaces. These
approaches make full use of structural information from the KGs
to model triple plausibility with a well-designed score function and
learn the entity/relation embeddings in a self-supervised manner.
Moreover, PLM-based methods consider KGC as text-based tasks
by fine-tuning pre-trained language models [ 8]. The short textual
descriptions are organized as an input sequence and encoded by the
PLMs. KG-BERT [ 40] is the first PLM-based method that models
KGC as a binary text classification task. Subsequent works like
MTL-KGC [ 16] and StAR [ 35] have further improved KG-BERT by
introducing more training tasks such as relation classification and
triple ranking and more complex triple encoding strategy. PKGC
[21] utilizes manual prompt templates to capture the triple semantic.
Other methods like KGT5 [ 5,29] make a step on the generative KGC
[43] in a sequence-to-sequence paradigm with encoder-decoder
PLMs like T5 [ 27]. PLM-based methods leverage the power of PLM
but make the training process into text-based learning, which is
difficult to capture complex structure information in the KGs.
2.2 LLMs for KG research
In recent years, large language models (LLMs) [ 23,33,46] have
made rapid progress and demonstrated powerful capabilities in a
considerable number of text-related tasks [ 49]. LLMs are usually
pre-trained in an auto-regressive manner with next word prediction
task [ 4] and demonstrate strong capability on text comprehension
and generation. Among the research topics of LLM, integrating
LLM and KG [ 25] is a popular and important one. On the one hand,
hallucination [ 39,48] is widespread in LLMs which means LLMs
are lack factual knowledge and not interpretable. KGs that store
structured knowledge can mitigate such a phenomenon [ 10,15,
26] by introducing factual knowledge into LLMs. On the other
hand, LLMs can benefit KG-related tasks such as KGC [ 51,52],
entity alignment [ 47], and KGQA [ 1] by its powerful generation
capability. KGs for LLMs (KG4LLM) and LLMs for KGs (LLM4KG)
are both important research topics. We focus on applying LLMs in
the KGC task (LLM4KGC), which has not been carefully studied yet.
KGLLaMA [ 41] made the first step by vanilla instruction tuning
approach but it lacks in-depth and systematic exploration about