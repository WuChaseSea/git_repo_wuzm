Page label: 4
File name: 2310.06671v2.pdf
text:
Therefore, we provide a detailed discussion of how the existing
LLM paradigms can introduce local structural information about
KGs to further enhance the model performance. However, though
these approaches can work to some extent, they have obvious
drawbacks. This textbf fundamental approaches to incorporate
KG structural information focus on adding the neighborhood
information to the input prompt in the text form . However,
representing the KG structural information in text is not a good
choice, which may bring in more invalid or redundant information
to the prompt. It’s not scalable and effective to increase promptlength indefinitely because a long context will lead to both a decline
in model capability and high computational consumption. Besides,
we also have difficulty finding the structural information in the
KGs that is decisive for triple discrimination. These two problems
put us in a dilemma.
4 METHODLOGY
To solve such issues, we propose the Knowledge Prefix Adapter
(KoPA for short) to incorporate the KG structural information into
LLM for KGC. Figure 2 presents an intuitive view of KoPA. Firstly
we extract the structural information of entities and relations from
the KG through structural embedding pre-training, and then we
inform this structural information to LLM through a structural
prefix adapter into the input sequence S. The LLMMis further
fine-tuned with the structural-enhanced text sequence. We will
discuss the details in the next few sections about our design.
4.1 Structural Embedding Pre-training
Instead of adding text about the neighborhood information into
the input sequence, KoPA extracts the structural information of
the entities and relations by self-supervised structural embedding
pre-training. For each entity 𝑒∈E and each relation 𝑟∈R, we
learn a structural embedding 𝒆∈R𝑑𝑒,𝒓∈R𝑑𝑟respectively, where
𝑑𝑒,𝑑𝑟are the embedding dimensions. We encode the KG struc-
tural information in the embeddings and further adapt them into
the textual representation space of LLMs. Referring to the exist-
ing embedding-based KGC paradigm, we define a score function
F(ℎ,𝑟,𝑡)to measure the plausibility of the triple (ℎ,𝑟,𝑡). We adopt
the self-supervised pre-training objective by negative sampling [ 3]:
L𝑝𝑟𝑒=1
|T|∑︁
(ℎ,𝑟,𝑡)∈T
−log𝜎(𝛾−F(ℎ,𝑟,𝑡))
−𝐾∑︁
𝑖=1𝑝𝑖log𝜎(F(ℎ′
𝑖,𝑟′
𝑖,𝑡′
𝑖)−𝛾)(4)