Page label: 9
File name: 2310.06671v2.pdf
text:
Making Large Language Models Perform Better in
Knowledge Graph Completion Preprint, 2024,
6 CONCLUSION
In this paper, we systematically explore how to incorporate struc-
tural understanding ability into LLMs to make structure-aware
reasoning for KGC tasks. We extend the original LLM paradigms
and propose structure-aware ICL and IT methods to incorporate the
structural information by text. We further propose KoPA, a knowl-
edge prefix adapter to incorporate the pre-trained structural embed-
dings into the LLMs. We conduct triple classification experiments
to make comprehensive comparisons among the structure-aware
methods and demonstrate the outperforming results achieved by
KoPA. In the future, we plan to dive deep into LLM-based KGC and
think about a more unified framework to accomplish all the KGC
tasks with LLMs. Besides, we will also explore flexibly adapting
KGs into LLM-based downstream applications to make the LLMs
knowledgeable, reliable, and human-friendly.
REFERENCES
[1]Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-Augmented
Language Model Prompting for Zero-Shot Knowledge Graph Question Answer-
ing. CoRR abs/2306.04136 (2023).
[2]Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: a collaboratively created graph database for structuring human
knowledge. In SIGMOD Conference . ACM, 1247–1250.
[3]Antoine Bordes, Nicolas Usunier, Alberto García-Durán, Jason Weston, and Ok-
sana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-relational
Data. In NIPS . 2787–2795.
[4]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In
NeurIPS .
[5]Chen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam. 2022. Knowledge Is Flat:
A Seq2Seq Generative Framework for Various Knowledge Graph Completion. In
COLING . International Committee on Computational Linguistics, 4005–4017.
[6]Mingyang Chen, Wen Zhang, Yushan Zhu, Hongting Zhou, Zonggang Yuan,
Changliang Xu, and Huajun Chen. 2022. Meta-Knowledge Transfer for Inductive
Knowledge Graph Embedding. In SIGIR . ACM, 927–937.
[7]Zhuo Chen, Wen Zhang, Yufeng Huang, Mingyang Chen, Yuxia Geng, Hongtao
Yu, Zhen Bi, Yichi Zhang, Zhen Yao, Wenting Song, Xinliang Wu, Yi Yang, Mingyi
Chen, Zhaoyang Lian, Yingying Li, Lei Cheng, and Huajun Chen. 2023. Tele-
Knowledge Pre-training for Fault Analysis. In ICDE . IEEE, 3453–3466.
[8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: