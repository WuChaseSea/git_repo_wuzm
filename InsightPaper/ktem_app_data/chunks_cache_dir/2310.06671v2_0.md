Page label: 1
File name: 2310.06671v2.pdf
text:
Making Large Language Models Perform Better in
Knowledge Graph Completion
Yichi Zhang1,2, Zhuo Chen1,2, Lingbing Guo1,2, Yajing Xu1,2, Wen Zhang1,2and Huajun Chen1,2,3∗
1Zhejiang University
2Zhejiang University-Ant Group Joint Laboratory of Knowledge Graph
3Alibaba-Zhejiang University Joint Institute of Frontier Technology
Hang Zhou, China
{zhangyichi2022,huajunsir}@zju.edu.cn
ABSTRACT
Large language model (LLM) based knowledge graph completion
(KGC) aims to predict the missing triples in the KGs with LLMs.
However, research about LLM-based KGC fails to sufficiently har-
ness LLMs’ inference proficiencies, overlooking critical structural
information integral to KGs. In this paper, we explore methods to
incorporate structural information into the LLMs, with the over-
arching goal of facilitating structure-aware reasoning. We first
discuss on the existing LLM paradigms like in-context learning and
instruction tuning, proposing basic structural information injection
approaches. Then we propose a Knowledge Prefix Adapter (KoPA)
to fulfill this stated goal. The KoPA uses a structural pre-training
phase to comprehend the intricate entities and relations within KGs,
representing them as structural embeddings. Then KoPA communi-
cates such cross-modal structural information understanding
to the LLMs through a knowledge prefix adapter which projects
the structural embeddings into the textual space and obtains virtual
knowledge tokens positioned as a prefix of the input prompt. We
conduct comprehensive experiments and provide incisive analysis
concerning how the introduction of cross-modal structural informa-
tion would be better for LLM’s factual knowledge reasoning ability.
Our code and data are available at https://github.com/zjukg/KoPA.
CCS CONCEPTS
•Information systems →Information integration ;•Computing
methodologies→Natural language generation ;Semantic networks .
KEYWORDS
Knowledge Graphs, Knowledge Graph Completion, Large Language
Models, Graph-text Fusion, Cross-modal Adapter
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
Preprint, 2024,
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnn
Figure 1: A simple case of LLM-based KGC. Useful struc-
tural information that describes the surrounding informa-
tion about the entities can serve as auxiliary prompts and
guide the LLM to make correct decisions.
1 INTRODUCTION
Knowledge graphs (KGs) [ 2] are the quintessential wisdom essence
and key infrastructure of modern AI. KGs represent and store real-
world knowledge in the triple form: ( head entity, relation, tail entity ).
This structured format of knowledge triples offers significant advan-
tages across many AI fields such as recommendation systems [ 30],
question answering [ 42], and fault analysis [ 7]. However, there is a
pertinent drawback of KGs, whether manually curated or automat-
ically extracted. Their scope is restricted to observed knowledge,
resulting in an incomplete representation riddled with unobserved
or missing triples. This phenomenon motivates knowledge graph
completion (KGC), which aims to predict the missing triples and