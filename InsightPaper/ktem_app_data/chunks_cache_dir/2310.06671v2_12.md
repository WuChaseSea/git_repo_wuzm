Page label: 5
File name: 2310.06671v2.pdf
text:
prehensive comparison with three broad classes of baselines on
triple classification, which is an important subtask of KGC. The
KGC baselines can be divided into three parts: embedding-based
methods [ 3,31,34,38], PLM-based methods [ 21,40], and LLM-based
methods [ 41]. Besides, we further divide the LLM-based methods
into two categories: training-free methods and fine-tuning methods.
Training-free methods consist of ZSR and ICL while fine-tuning
methods consist of vanilla IT and structure-aware IT (enhanced IT).
The specific models used for these baselines are listed below:
(1).Embedding-based KGC methods . We select four tradi-
tional embedding-based KGC methods for comparisons, namely
TransE [], DistMult [ 38], ComplEx [ 34], and RotatE [ 31]. These
methods predict the triple plausibility by the learned structural
embeddings and the score functions defined in the model.
(2).PLM-based KGC methods . We select KG-BERT [ 40] and
PKGC [ 21] as PLM-based KGC baselines, which are classic methods
focusing on the triple classification task. These methods treat triple
classification as a binary text classification task.
(3).LLM-based KGC methods . LLM-based KGC research is
still at an early stage. There are only KGLLaMA [ 41] to be the
LLM-based KGC baseline. In addition to KGLLaMA, the methods
proposed in Section 3 by us including ZSR, ICL, IT, and structure-
aware IT (enhanced IT) will also serve as baselines.