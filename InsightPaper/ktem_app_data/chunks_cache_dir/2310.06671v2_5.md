Page label: 3
File name: 2310.06671v2.pdf
text:
Making Large Language Models Perform Better in
Knowledge Graph Completion Preprint, 2024,
as images [ 19,50], audio [ 22], and video [ 22], which are also called
multi-modal LLMs [ 45]. These methods tend to encode non-textual
information through the modality encoders and then process it as
virtual text tokens. The non-textual tokens are aligned with the
word tokens by instruction tuning on multi-modal datasets.
The multi-modal LLM mentioned above usually excludes graph,
which is another important data modality. There are also some
works talking about how to incorporate graph data into LLMs.
Drug-Chat [ 17] proposes to encode the drug molecule graphs with
graph encoders and fine-tune the LLM to predict drug interactions.
Other works [ 11,18,36,44] explore how to solve graph learning
tasks like node classification and graph classification by convert
the graph structure information into LLMs.
Our research is relative to this topic as KGs also have complex
graph structures on top of the text descriptions. In this paper, we
will explore how to incorporate complex structural information in
the KGs into the LLMs to achieve better reasoning capabilities on
knowledge graph completion.
3 BASIC SETTINGS FOR LLM-BASED KGC
3.1 Notations and Preliminaries
A KG can be denoted as G=(E,R,T,D)whereE,Rare the entity
set, relation set respectively. T={(â„,ğ‘Ÿ,ğ‘¡)|â„,ğ‘¡âˆˆE,ğ‘ŸâˆˆR} is the
triple set andDis the description set of each entity and relation. We
denoteD(ğ‘’),D(ğ‘Ÿ)as the short textual description of each entity
ğ‘’âˆˆEand each relation ğ‘ŸâˆˆR. For example, the text description of
the entity â€™/m/0ctzf1â€™ is D(â€™/m/0ctzf1â€™)=â€™The Transformersâ€™. When
applying LLMs to KGC tasks, we denote a LLM as Mthat serves as a
text decoder. The input textual sequence Sof the modelMconsists
of several parts: the instruction prompt I, the triple promptX, and
the optional auxiliary demonstration prompt U. The instruction
promptIis the manually prepared instruction to guide the LLM M
to execute the KGC task. The triple prompt Xcontains the textual
information about the triples that need to be processed, which can
be denoted asX(â„,ğ‘Ÿ,ğ‘¡)=D(â„)âŠ•D(ğ‘Ÿ)âŠ•D(ğ‘¡), where(â„,ğ‘Ÿ,ğ‘¡)âˆˆT
is a triple andâŠ•denotes the textual token concatenation operation.
In other words, the short descriptions of â„,ğ‘Ÿ,ğ‘¡ would be applied as
the input information. The auxiliary demonstration prompt Uis
an optional prompt for different settings. In the following, we will
follow this set of notations.
Meanwhile, we use triple classification as an entry point to in-
vestigate how to utilize LLM to accomplish the KGC task. Triple
classification is a basic KGC task aiming to conduct binary classi-
fication tasks on the given triples. Whereas in the LLM paradigm,
all tasks are converted into the form of text generation. Therefore,
we desire the model Mto answer true or false given the textual
sequence inputS=IâŠ•UâŠ•X .
Triple classification is different from vanilla text classification
because the entities and the relations in the prompt have complex
semantic information defined by the given KG. Without knowledge
of this type of information, the model response is unreliable and
unstable. Despite the vast amount of commonsense knowledge
that exists in the LLMs [ 48], research has shown that large models