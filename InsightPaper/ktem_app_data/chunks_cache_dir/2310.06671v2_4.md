Page label: 2
File name: 2310.06671v2.pdf
text:
pre-trained in an auto-regressive manner with next word prediction
task [ 4] and demonstrate strong capability on text comprehension
and generation. Among the research topics of LLM, integrating
LLM and KG [ 25] is a popular and important one. On the one hand,
hallucination [ 39,48] is widespread in LLMs which means LLMs
are lack factual knowledge and not interpretable. KGs that store
structured knowledge can mitigate such a phenomenon [ 10,15,
26] by introducing factual knowledge into LLMs. On the other
hand, LLMs can benefit KG-related tasks such as KGC [ 51,52],
entity alignment [ 47], and KGQA [ 1] by its powerful generation
capability. KGs for LLMs (KG4LLM) and LLMs for KGs (LLM4KG)
are both important research topics. We focus on applying LLMs in
the KGC task (LLM4KGC), which has not been carefully studied yet.
KGLLaMA [ 41] made the first step by vanilla instruction tuning
approach but it lacks in-depth and systematic exploration about
how to unleash the power of KGs themselves to make structure-
aware reasoning in LLMs and achieve better KGC performance. In
this paper, we will dive into this problem from a more systematic
perspective with the knowledge graph completion task.
2.3 Incorporate Non-textual Modality
Information into LLMs
As LLMs demonstrate generalizable capabilities on text generation,
many other works attempt to incorporate non-textual modality such