Page label: 7
File name: 2310.06671v2.pdf
text:
w/ ComplEx 81.21 82.12
w/ Random 81.53 82.36
PositionInfix 81.21 82.69
Suffix 77.29 77.75
of KoPA, we conduct a new transferability experiment. In this
experiment, we will demonstrate that the knowledge prefix adapter
will learn to transfer from structural embeddings to textual token
representations and provide semantic-rich auxiliary information to
enhance the decoding process of LLM inference.
We demonstrate this point by testing the influence of KoPA for
entities that do not appear in the training phase, which is also called
inductive setting in other KGC works [ 6]. We split the KG dataset
into an inductive setting with a defined inductive rate (IR), which
refers to the ratio of unseen entities during training. For example,
if IR=10%, we will randomly select 10% entities as the inductive
entity set. Any triple in the training set whose head or tail is in the
inductive set will be removed during training. Besides, the triples
in the test set will be divided into two parts: the seen (S) part and
the unseen (U) part. If the head or tail in a triple is in the inductive
entity set, it will be regarded as unseen. We fine-tune the LLM with
only remaining seen triples and test on both seen and unseen triples.
In this setting, a set of entities will not participate in the training
process and the LLM does not see their textual descriptions, which
will make the test process more challenging. We report the accuracy
and F1 score for seen (S), unseen (U), and all (A) test triples, which
is shown in Figure 3 for three fine-tuning methods: KoPA, vanilla
IT, and structure-aware IT (enhanced IT in the figure).
From the radio charts, we can observe that KoPA outperforms the
other methods for unseen triples and has less performance degra-
dation when IR increases. The performance of structure-aware
IT (enhanced IT) with neighborhood triples in the textual form
is more unstable. These phenomena suggest that the knowledge
prefix adapter can learn a good mapping from the structural em-
beddings to the textual representation, which is transferable even if
the entities are unseen during training. The structural embeddings
captured from KG play a more significant role in informing the
LLM with useful structural information.
5.4 Ablation Study
To verify the effectiveness of the KoPA design, we conduct a two-
part ablation study. The first part is designed to verify the effec-
tiveness of structural embedding and the second part is designed
to verify the effectiveness of prefix adapter. As shown in Table 4,
we can find that removing the structural embeddings or replacing
them with random initialized embeddings both lead to performance
decline. Also, we find that the model is compatible with different