Page label: 8
File name: 2310.06671v2.pdf
text:
Preprint, 2024, Yichi Zhang et al.
Enhanced ITKoPAVanilla ITRotatE
Figure 4: The Venn diagram of the correct predictions from
various KGC models. Each intersecting part in the diagram
represents the same predictions from different models on
certain data.
types of structural embeddings. However, the performance gain
depends on whether the embedding was originally powerful in the
triple classification task or not. Refer to Tables 3, TransE [ 3] and Ro-
tatE [ 31] are better embedding-based KGC models compared with
DistMult [ 38] and ComplEx [ 34]. This demonstrates that semantic-
rich structural information is the key to performance improvement
and KoPA takes full advantage of it.
Meanwhile, putting the virtual knowledge tokens generated by
the adapter in the middle (infix) or in the last (suffix) of the input
sequence will also decrease the performance. We believe the reason
is that putting tokens in the front of the sequence will make all
the text pay attention to them as LLMs are usually decoder-only
architectures with unidirectional self-attention. Then the LLM can
make a better decision with the structural embeddings that fully
interact with the text. Combining these two parts of the ablation
study, we believe that our design of KoPA is effective and reasonable.
5.5 Case Study
To make a more intuitive view of KoPA, we conduct a case study in
this section from both macro and micro perspectives. From a macro
perspective, we count the prediction overlap of several models and
plot a Venn diagram shown in Figure 4.
From the diagram we can find that KoPA has a significant por-
tion of the proper predictions that do not intersect with several
other models, which means that KoPA makes the right prediction
on some test data that many other models predict incorrectly. This
suggests that the structural information incorporated in KoPA has a
significant role in making correct predictions. For a micro example,
a test triple ( John Landis, film director film, Coming to America ) is
predicted as wrong by the RotatE model and vanilla instruction tun-
ing LLM. With retrieved neighborhood triples ( Coming to America,
locations, New York City ), (John Landis, nationality, USA ), (Coming
to America, genre, romantic comedy ), (Comedy, common netflix ti-
tles, Coming to America ), the structure-aware fine-tuned LLM still
makes a wrong prediction because the neighborhood information
is of little use in the judgment of the current prediction though
they are the correct factual. The structural embeddings applied
in KoPA contain more information than structural information in
the form of text and are easier for us to extract by a structural
pre-training process. Thus, KoPA outperforms other models in the
triple classification task.
CSClinicalHumanity
EconomicsPolitics
KnowPATNonePROAFTRRHFSocial Science
STEMOtherAverage
NoneUMLSCoDeXFB15K-237NFigure 5: The common ability experiments on MMLU.
5.6 Common Ability Retention
To delve into the preservation of generic capabilities in LLMs, we
conducted another experiment to assess the overall proficiency of
LLMs both before and after fine-tuning. We apply the MMLU [ 12]
benchmark for this problem. MMLU is the most popular bench-
mark to evaluate the general abilities of LLMs in different domains
such as Humanities, Social Sciences, STEM, and others. The overall
evaluation results on different datasets are shown in Figure 5:
From the results, it can be noticed that after KoPA training, there
were discernible alterations in the generalized abilities of LLMs.
In most instances, there was a decrease, but notably, STEM profi-
ciency exhibited improvement on the UMLS dataset. We attribute
this phenomenon to the UMLS being a medical KG, encompassing