Page label: 7
File name: 2310.06671v2.pdf
text:
Making Large Language Models Perform Better in
Knowledge Graph Completion Preprint, 2024,
(a). IR=10%(b). IR=20%
(c). IR=30%(d). IR=40%KoPAVanilla ITEnhanced IT
Figure 3: The results of the transferbility experiment. We
report the results on CoDeX-S dataset under different induc-
tive rate (IR). Besides, we split the test data into seen (S) and
unseen (U) parts based on whether the entity appeared dur-
ing training. Also we total the results of all (A) the test data
together. Accuracy (Acc) and F1-score (F1) are reported in the
radar charts.
original embedding-based RotatE method, especially on larger and
more challenging datasets like CoDeX-S and FB15K-237N.
Meanwhile, compared with all LLM-based approaches, we can
see that the LLMs cannot understand the KG structural information
well without fine-tuning. The zero-shot LLMs perform very poorly
in the triple classification task even though GPT-3.5-turbo (175B
parameters) has excellent capability. Though the demonstrations
provided by ICL can incorporate the KG information, the perfor-
mance gain is limited. Besides, the prediction results of training-free
methods are biased and easy to slip into the extremes of all-right
or all-wrong, as the recall of them is either very high or very low
but the F1 scores are relatively low all the time.
However, fine-tuning LLMs can introduce the KG information
into LLMs as the overall performance makes obvious improvements.
Meanwhile, though structure-aware IT enhances the input prompt
with neighborhood information of triples, its performance is also
limited compared with KoPA. This suggests that the structural
embeddings consist of more semantic-rich information compared
with text-based auxiliary prompts, which can also be understood
by the LLM through the prefix adapter. Combining the analysis
in Section 4.3 and the experimental results, KoPA achieves better
results on top of shorter prompts.
5.3 Transferability Exploration
The results in the main experiments have shown the effectiveness
of KoPA. To further validate the generality and the transferabilityTable 4: Ablation study results on CoDeX-S. We first replace
the pre-trained structural embedding with other components
and change the insert position of virtual knowledge tokens to
demonstrate the effectiveness of knowledge prefix adapter.
Model Acc F1
KoPA(Prefix + RotatE) 82.74 84.11
Embeddingw/o SE 81.18 82.52
w/ TransE 82.46 83.42
w/ DistMult 80.71 81.27
w/ ComplEx 81.21 82.12
w/ Random 81.53 82.36
PositionInfix 81.21 82.69
Suffix 77.29 77.75
of KoPA, we conduct a new transferability experiment. In this
experiment, we will demonstrate that the knowledge prefix adapter
will learn to transfer from structural embeddings to textual token
representations and provide semantic-rich auxiliary information to
enhance the decoding process of LLM inference.
We demonstrate this point by testing the influence of KoPA for
entities that do not appear in the training phase, which is also called
inductive setting in other KGC works [ 6]. We split the KG dataset
into an inductive setting with a defined inductive rate (IR), which
refers to the ratio of unseen entities during training. For example,
if IR=10%, we will randomly select 10% entities as the inductive
entity set. Any triple in the training set whose head or tail is in the
inductive set will be removed during training. Besides, the triples
in the test set will be divided into two parts: the seen (S) part and