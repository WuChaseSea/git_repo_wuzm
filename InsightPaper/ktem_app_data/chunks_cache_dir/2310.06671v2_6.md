Page label: 3
File name: 2310.06671v2.pdf
text:
is a triple andâŠ•denotes the textual token concatenation operation.
In other words, the short descriptions of â„,ğ‘Ÿ,ğ‘¡ would be applied as
the input information. The auxiliary demonstration prompt Uis
an optional prompt for different settings. In the following, we will
follow this set of notations.
Meanwhile, we use triple classification as an entry point to in-
vestigate how to utilize LLM to accomplish the KGC task. Triple
classification is a basic KGC task aiming to conduct binary classi-
fication tasks on the given triples. Whereas in the LLM paradigm,
all tasks are converted into the form of text generation. Therefore,
we desire the model Mto answer true or false given the textual
sequence inputS=IâŠ•UâŠ•X .
Triple classification is different from vanilla text classification
because the entities and the relations in the prompt have complex
semantic information defined by the given KG. Without knowledge
of this type of information, the model response is unreliable and
unstable. Despite the vast amount of commonsense knowledge
that exists in the LLMs [ 48], research has shown that large models
are numb to fine-grained factual knowledge and will fall into a
hallucination. Thus, incorporating the KG information into the
prompt to provide more auxiliary information and guide the LLMto make structure-aware reasoning is the key to achieving excellent
LLM-based KGC.
3.2 Extending Existing LLM Paradigms
In this section, we first discuss how to solve the KGC task with
existing mainstream LLM paradigms called training-free reasoning
approaches and instruction-tuning approaches.
3.2.1 Training-free reasoning approaches .Training-free rea-
soning approaches prompt the LLMs to get direct answers without
training. Common training-free methods consist of zero-shot rea-
soning (ZSR) and in-context learning (ICL). For ZSR, we directly
utilize the sequence Sğ‘§ğ‘ ğ‘Ÿ=IâŠ•X as the input to get the prediction
results. The decoding process of the LLM Mcan be formulated as:
Ağ‘§ğ‘ ğ‘Ÿ=arg max
Ağ‘ƒM(A|Sğ‘§ğ‘ ğ‘Ÿ)
=arg max
Ağ‘ƒM(A|Iğ‘§ğ‘ ğ‘Ÿ,X)(1)
whereAis the generated answer of the model MandIğ‘§ğ‘ ğ‘Ÿis
the instruction template for ZSR. In the setting of ZSR, no KG
information is added to the input sequence Sğ‘§ğ‘ ğ‘Ÿ. The determinative
information in the ZSR prompt is only the textual descriptions of
the test triple. ZSR is unable to incorporate KG information due to
its setting limitations, otherwise, it cannot be called zero-shot.
As another training-free paradigm, in-context learning (ICL) [ 9]
allows the modelMto add auxiliary demonstration Uto the input
Sand accomplish the task in the form of analogical reasoning,
which can be denoted as:
Ağ‘–ğ‘ğ‘™=arg max
Ağ‘ƒM(A|Sğ‘–ğ‘ğ‘™)
=arg max
Ağ‘ƒM(A|Iğ‘–ğ‘ğ‘™,U,X)(2)
As for the triple classification task, the demonstration Ushould
be some triples and their labels in the form of {(Xğ‘–,ğ‘¦ğ‘–),1â‰¤ğ‘–â‰¤ğ‘˜},
whereXğ‘–is the demonstration triple and ğ‘¦ğ‘–is the label. We denote
the ICL with ğ‘˜demonstrations as ğ‘˜-shot ICL.
The demonstration triples can be randomly sampled from the
existing training KG. However, to further incorporate the relative