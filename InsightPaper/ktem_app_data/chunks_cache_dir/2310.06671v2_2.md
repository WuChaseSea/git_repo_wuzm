Page label: 2
File name: 2310.06671v2.pdf
text:
Preprint, 2024, Yichi Zhang et al.
are often overlooked. This richly non-textual structured infor-
mation , if properly incorporated, can significantly enhance the
LLM’s understanding and representation of KGs. Figure 1 presents
an intuitive view of the importance of structural information for
LLM reasoning. However, this is neglected by vanilla ZSR and IT
approaches [ 41] because each input typically only includes a single
input triple, leading to potential wastage of the structural informa-
tion inherent in the KG. Such an approach fails to equip the LLMs
with the awareness of the KG structure.
To address these issues, we take a strategic step to LLM-based
KGC, aiming to explore how to incorporate the KG structural infor-
mation into the LLMs and enable structure-aware reasoning. Our
initial focus involves transferring the existing LLM paradigms such
as in-context learning (ICL) [ 9] and instruction tuning (IT) [ 24] to a
structure-aware context. We propose a structure-aware ICL method
and a structure-aware IT method as the base models, focusing on
integrating the KG structural information into LLM through text
form. Such an approach benefits from the fact that specific textual
information exists about entities and relationships in KG so that
we can use the text to represent this knowledge as complementary
background information, expecting that LLMs can learn the local
structural information in KG through textual prompts. But they also
have the obvious disadvantage that there is a clear semantic divide
between structural and textual information. The textual descrip-
tions in the expanded prompt still fail to fully exploit the structural
information in the complex KG.
Additionally, we propose a novel Knowledge Prefix Adapter
(KoPA) approach to make LLMs a better knowledge reasoner, lever-
aging structural embedding pre-training to capture the KG
structural information . Then KoPA transforms the structural
embeddings into textual embedding space by a knowledge prefix
adapter and obtains several virtual knowledge tokens. These to-
kens, acting as prefixes in the input prompt sequence , direct
the instruction-tuning process, providing valuable supplementary
input triple information. This mapping of structural embeddings to
textual form provides auxiliary information to input triples. Besides,
we conduct comprehensive analysis and experiments, highlight-
ing the remarkable performance and transferability of KoPA. In
summary, our contribution is three-folded:
•Extending the existing LLM paradigms. We are the first
extensive investigation of LLM-based KGC methods, specifically
by incorporating KG structural information to enhance the rea-
soning ability of LLMs. We discuss the pipeline to adapt the
existing LLM paradigms like ICL and IT to a structure-aware
setting for KGC using addtional textual prompts.
•Designing new cross-modal LLM paradigm. We further pro-
pose a knowledge prefix adapter (KoPA) that effectively inte-
grates pre-trained KG structural embeddings with LLMs. KoPA
fosters comprehensive cross-modal interactions between tex-
tual embeddings from LLMs and structural embeddings sourced
from KGs to enhance LLM’s reasoning ability.
•Comprehensive evaluation. We conduct extensive experi-
ments on three public benchmarks and evaluate the KGC per-
formance of all the structure-aware methods proposed by us
with adequate baseline comparison with further exploration of
the transfer ability and knowledge retention degree.2 RELATED WORKS
2.1 Knowledge Graph Completion
Knowledge graph completion (KGC) [ 37] is an important topic in
the KG community, aiming to mine unobserved triples in a given
KG. KGC contains several sub-tasks such as triple classification
[3], entity prediction [ 3]. The common point among KGC tasks is
to establish an effective mechanism to measure the plausibility of
the triples. The mainstream KGC methods can be divided into two
categories: embedding-based and PLM-based methods. Embedding-