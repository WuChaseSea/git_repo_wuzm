Page label: 4
File name: 2310.06671v2.pdf
text:
Preprint, 2024, Yichi Zhang et al.
Figure 2: An overview of the knowledge prefix adapter (KoPA) by us. KoPA first pre-trains structural embeddings for the
entities and relations in the given KG and then employs instruction tuning to fine-tune the LLM. The structural embeddings of
the given input triple will be projected into the textual space of the LLM by the adapter and serve as prefix tokens in the front
of the input sequence, which can be "seen" by the following textual tokens due to the unidirectional attention mechanism in
the decoder-only LLM.
No other auxiliary demonstrations are included in the input tem-
plate. To train the model M, the input sequence is organized as
S𝑖𝑡=I𝑖𝑡⊕X⊕A𝑖𝑡, whereA𝑖𝑡is the predicted answer of the
training data. The model Mis fine-tuned with the next word pre-
diction task [ 49] which is a universal approach to training LLMs.
The training objective can be formulated as:
L𝑖𝑡=−1
|S𝑖𝑡||S𝑖𝑡|∑︁
𝑖=1log𝑃M(𝑠𝑖|𝑠<𝑖) (3)
where𝑠𝑖(𝑖=1,2,...,|S𝑖𝑡|)represents the textual tokens of the
input sequenceS𝑖𝑡. In the inference stage, the model Mis employed
to predict the answer A𝑖𝑡of the test data like Equation 1. Besides,
negative sampling [ 21] is also applied to generate negative data
samples as training KG only consists of positve triples.
To incorporate semantic-rich KG information into LLMs, we
also propose a structure-aware instruction tuning approach by
adding the one-hop neighborhood structure information in the
input prompt to inform the LLM with the local structural informa-
tion. As mentioned before, the structural information of KG plays a
significant role in the KGC tasks [ 37]. To incorporate such KG infor-
mation during the fine-tuning stage, we achieve this goal by adding
the neighborhood descriptions of the input triple. Specifically, we
can sample the neighborhoods of the head ℎand tail𝑡and put the
textual descriptions of neighborhood triples in the demonstration
promptU𝑖𝑡. In this way, the input training sequence is enhanced
asS𝑖𝑡=I𝑖𝑡⊕U𝑖𝑡⊕X⊕A𝑖𝑡.
Therefore, we provide a detailed discussion of how the existing
LLM paradigms can introduce local structural information about
KGs to further enhance the model performance. However, though
these approaches can work to some extent, they have obvious
drawbacks. This textbf fundamental approaches to incorporate
KG structural information focus on adding the neighborhood
information to the input prompt in the text form . However,
representing the KG structural information in text is not a good
choice, which may bring in more invalid or redundant information
to the prompt. It’s not scalable and effective to increase promptlength indefinitely because a long context will lead to both a decline
in model capability and high computational consumption. Besides,
we also have difficulty finding the structural information in the
KGs that is decisive for triple discrimination. These two problems
put us in a dilemma.
4 METHODLOGY
To solve such issues, we propose the Knowledge Prefix Adapter
(KoPA for short) to incorporate the KG structural information into
LLM for KGC. Figure 2 presents an intuitive view of KoPA. Firstly
we extract the structural information of entities and relations from
the KG through structural embedding pre-training, and then we