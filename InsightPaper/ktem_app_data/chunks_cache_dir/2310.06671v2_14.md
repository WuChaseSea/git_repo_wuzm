Page label: 6
File name: 2310.06671v2.pdf
text:
ICL(2-shot) 53.78 52.47 80.18 63.43 52.95 51.54 98.85 67.75 57.81 56.22 70.56 62.58
ICL(4-shot) 53.18 52.26 73.22 60.99 51.14 50.58 99.83 67.14 59.29 57.49 71.37 63.68
ICL(8-shot) 55.52 55.85 52.65 54.21 50.62 50.31 99.83 66.91 59.23 57.23 73.02 64.17
LLM-based
Fine-tuningKG-LLaMA [41] 85.77 87.84 83.05 85.38 79.43 78.67 80.74 79.69 74.81 67.37 96.23 79.25
KG-Alpaca [41] 86.01 94.91 76.10 84.46 80.25 79.38 81.73 80.54 69.91 62.71 98.28 76.56
Vanilla IT 86.91 95.18 77.76 85.59 81.18 77.01 88.89 82.52 73.50 65.87 97.53 78.63
Structure-aware IT 89.93 93.27 86.08 89.54 81.27 77.14 88.40 82.58 76.42 69.56 93.95 79.94
KoPA 92.58 90.85 94.70 92.70 82.74 77.91 91.41 84.11 77.65 70.81 94.09 80.81
5.1.3 Implementation and Detail Settings .We reproduce the
baseline results and implement the KoPA proposed by us.
For embedding-based KGC methods, we reproduce the results
with OpenKE we set the embedding dimension ùëëùëí=ùëëùëü=512and
sampleùêæ=32negative samples during training. The margin ùõæis
tuned among{0,4,6,8,12}. After training KGC models, we search
for the best classification score threshold on the validation set for
test data following the traditional setting [3].
For PLM-based methods, the backbone model for PLM-based
KGC methods is BERT [ 8]. We fine-tune the KG-BERT according
to the official code implementation. Since PKGC requires a lot of
manual work to annotate each relation with a prompt, we only
report the results of FB15K-237N shown in the original paper.
For zero-shot reasoning, in addition to measuring with the same
backbone Alpaca, we also test the performance of the GPT-3.5-turbor
which has 175B parameters. For the in-context learning method,
we sample k-shot (k=1,2,4,8) structure-aware demonstrations. Be-
sides, we sample 4 neighborhood triples for each triple to conduct
structure-aware instruction tuning. For KoPA, we employ RotatE
[31] and the score function of structural embedding pre-training
and the embedding dimension is set to 512 and the adapter is a
512√ó4096 linear projection layer.
For KoPA, we employ Alpaca-7B [ 32] as the LLM backbone. Al-
paca is a famous extended version of LLaMA [ 33] model fine-tuned
on instruction-following data. We reproduce the triple classification
results of KGLLaMA [ 41] over two backbones (LLaMA and Alpaca)