Page label: 5
File name: 2310.06671v2.pdf
text:
Making Large Language Models Perform Better in
Knowledge Graph Completion Preprint, 2024,
Table 1: Comparasion among LLM-based KGC methods in
three ways. As for the prompt length anaysis, ğ¿ğ¼,ğ¿ğ‘‡denote
the length of the instruction prompt and triple prompt. ğ¿ğ·
denotes the length of a demonstration and ğ‘˜is the demon-
stration number. ZSR/ICL/IT refer to zero-shot reasoning,
in-context learning, and instruction tuning respectively.
MethodRequires
Fine-tuningExtra
KG InfoPrompt
Length
ZSR % % ğ¿ğ¼+ğ¿ğ‘‡
ICL % ! ğ¿ğ¼+ğ¿ğ‘‡+ğ‘˜ğ¿ğ·
Vanilla IT ! % ğ¿ğ¼+ğ¿ğ‘‡
Enhanced IT ! ! ğ¿ğ¼+ğ¿ğ‘‡+ğ‘˜ğ¿ğ·
KoPA ! ! ğ¿ğ¼+ğ¿ğ‘‡+3
whereğ›¾is the margin, ğœis the sigmoid activation function and
(â„â€²
ğ‘–,ğ‘Ÿâ€²
ğ‘–,ğ‘¡â€²
ğ‘–)(ğ‘–=1,2,...,ğ¾)areğ¾negative samples [ 3] of(â„,ğ‘Ÿ,ğ‘¡).
The weight ğ‘ğ‘–is the self-adversarial weights proposed in [31].
By minimizing such a pre-training loss, the structural embed-
dings of each entity and relation are optimized to fit all its relative
triples thus the KG structural information such as subgraph struc-
ture and relational patterns is captured in the embeddings. Such
an approach has been proven effective in many embedding-based
KGC methods [ 3,31] to capture classic structural information like
relational patterns and distributed entity representations [ 13] in
the earliest days.
4.2 Knowledge Prefix Adapter
After structural embedding pre-training, we could obtain the struc-
tural embeddings(ğ’‰,ğ’“,ğ’•)of a triple(â„,ğ‘Ÿ,ğ‘¡)where the KG structural
information is encoded in. However, the structural embeddings are
learned in a different representation space against the textual to-
ken representation space of the LLM M, which meansMcan not
directly understand these embeddings. Thus we apply a knowledge
prefix adapterPto project them into the textual token representa-
tion space ofM. Specifically speaking, the structural embeddings
are converted to several virtual knowledge tokens KbyP:
K=P(ğ’‰)âŠ•P( ğ’“)âŠ•P( ğ’•) (5)
In practice, the adapter Pwould be a simple projection layer
[50]. Then we putKin the front of the original input sequence
Sserving as a prefix of the instruction and triple prompt Sğ‘˜ğ‘ğ‘=
KâŠ•Iğ‘–ğ‘¡âŠ•X. This way, all the following text tokens can be seen
with the prefixKdue to the unidirectional attention in decoder-
only LLMs. By doing this, the textual tokens can pay unidirectional
attention to the structural embeddings of the input triple. Such
a structure-aware prompt will be employed during fine-tuning
and inference. During training, we froze the pre-trained structural
embeddings. The adapter is optimized to learn the mapping from
structural knowledge toward textual representation and will have
the generalization to new triples in the inference stage, which will
benefit the textual description and provide the triple information
from another perspective to make enhanced predictions.Table 2: Statistical information of datasets. The positve (+)
and negative (-) samples are 1:1 in the valid / test set.