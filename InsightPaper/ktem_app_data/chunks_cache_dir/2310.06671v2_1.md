Page label: 1
File name: 2310.06671v2.pdf
text:
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnn
Figure 1: A simple case of LLM-based KGC. Useful struc-
tural information that describes the surrounding informa-
tion about the entities can serve as auxiliary prompts and
guide the LLM to make correct decisions.
1 INTRODUCTION
Knowledge graphs (KGs) [ 2] are the quintessential wisdom essence
and key infrastructure of modern AI. KGs represent and store real-
world knowledge in the triple form: ( head entity, relation, tail entity ).
This structured format of knowledge triples offers significant advan-
tages across many AI fields such as recommendation systems [ 30],
question answering [ 42], and fault analysis [ 7]. However, there is a
pertinent drawback of KGs, whether manually curated or automat-
ically extracted. Their scope is restricted to observed knowledge,
resulting in an incomplete representation riddled with unobserved
or missing triples. This phenomenon motivates knowledge graph
completion (KGC), which aims to predict the missing triples and
further enhance the given KG.
Existing KGC approaches can be divided into two categories:
methods based on embeddings [ 3] and pre-train language models
(PLM) [ 40]. Recently, as large language models (LLMs) [ 23,46] show
outperforming capabilities [ 24], this field has recently been revo-
lutionized by LLMs. Some works [ 41] make the first step towards
LLM-based KGC, employing existing paradigms like zero-shot rea-
soning (ZSR) [ 4] and instruction tuning (IT) [ 24] to accomplish
the KGC task. However, such approaches transform the KGC task
into a text-based prediction of individual triples, leading to specific
fundamental problems. LLMs lack the depth and precision of factual
knowledge which always results in the hallucination [ 48] problem
of LLMs. Besides, the structural intricacies of KGs such as sub-
graph structure, relational patterns, and relative entities/relationsarXiv:2310.06671v2  [cs.CL]  14 Apr 2024