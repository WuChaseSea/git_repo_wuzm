# 大语言模型参数
llm_name: "qwen"
llm_embed_type: 3 # 最终的上下文文档编码参数

# 数据参数
data_path: "/Users/wuzm/Documents/CodeRepository/git_repo_wuzm/EasyRAG/data/format_data_with_img"

# 分块策略
split_type: 0 # 0-->Sentence 1-->Hierarchical
chunk_size: 1024
chunk_overlap: 200

# 密集检索器参数
qdrant_url: "http://localhost:6333"
reindex: false
embedding_name: "embedding_models/bge-small-zh-v1.5"
vector_size: 512
cache_path: "cache"
collection_name: "ainops24"

# 粗排参数
re_only: false  # 只检索，用于调试检索
retrieval_type: 2 # 1-->密集 2-->稀疏 3-->混合
f_topk: 256 # 仅用于混合检索器最终的fusion数量
f_topk_1: 288
f_topk_2: 192
f_topk_3: 6

# 文本排序编码方式
f_embed_type_1: 1 # 密集检索的文档编码方式
f_embed_type_2: 2 # 稀疏检索的文档编码方式
r_embed_type: 1 # 重排的文档编码方式

# 稀疏检索器参数
bm25_type: 0 # 0-->官方实现 1-->bm25s实现，速度更快

# 重排器参数
r_topk: 6 # 精排topk
r_topk_1: 6 # 精排后Fusion的topk
reranker_name: ../models/bge-reranker-v2-minicpm-layerwise
use_reranker: 0 # 0-->不使用 1-->ST的普通Reranker 2-->bge LLM Reranker
r_embed_bs: 32
r_use_efficient: 0 # 0-->不加速 1-->使用最大值选择方法加速 2-->使用熵选择方法加速

# 上下文压缩参数
compress_method: "" # bm25_extract llmlingua longllmlingua
compress_rate: 0.5

# 流程参数
rerank_fusion_type: 0 # 0-->不使用精排后fusion 1-->两路检索结果rrf 2-->生成长度最大的作为最终结果 3-->两路生成结果拼接
ans_refine_type: 0 # 0-->不对答案做后处理 1-->LLM利用top1文档和参考答案生成新答案 2-->LLM将top1文档和参考答案拼接生成新答案

# 虚假文档参数
hyde: false
hyde_merging: false

